---
layout: post
title: Day17
hide_title: False
excerpt: Boost Camp AI Tech - DAY 17
use_math: true
toc: true
toc_sticky: true
# toc_label: Category
feature-img: assets/img/feature-img/story.jpeg
author: Jay
tags: [Boost camp, AI tech, Day17]
---

# Day 17

## 학습 내용
  - [x] NLP
    - [x] RNN
    - [x] LSTM
    - [x] GRU

<br> 

## 개인 학습
---
### RNNs
Types of RNNs
![typesofrnns](/img/types_of_rnns.png)
> one to one : Standard Neural Networks
> one to many : Image Captioning
> many to one : Sentiment Classification
> many to many(with blank) : Machine Translation
> many to many : Video Classification on Frame Level

#### Character-level Language Model
앞서 살펴보았던 RNNs에서 many to many의 방법을 활용하여 하나의 글자(character)가 입력으로 주어졌을 때 다음 글자를 예측하는 모델을 구현하는 방법에 대해 알아보도록 하겠습니다.   

![traininghello](/img/traininghello.png)

RNNs의 hidden 벡터를 계산하는 식을 살펴보면 다음과 같습니다.  
> $h_t = tanh(W_{hh}h_{t-1} + W_{xh}x_t)$
> $y_t = W_{hy}h_t$

위 모식도에서 output layer가 softmax를 거쳐 다음 ground truth, 즉 다음에 나올 단어의 벡터를 가리킬 수 있도록 $W_{hh}, W_{xh}, W_{hy}$를 학습합니다.   

여기서 RNNs 같은 경우 Forward를 구하고자 하는 글자 수 만큼 진행하는 경우 loss를 구하기 위해서 모든 sequence를 계산해야하는 문제가 생기며, 이는 gradient를 계산하기 위한 backpropagation의 계산에도 영향을 미칩니다. 한 번에 너무 많은 양을 계산하게 될 경우 계산을 제대로 진행할 수 없기 때문에 일정 sequence마다 나누어 loss를 계산하게 됩니다. 이 경우 나눈 sequence를 chunk of sequence라고 부릅니다.   

<br>

### LSTM
LSTM은 기존의 RNNs가 sequence에 따라 계속해서 weight를 곱하는 방식을 가지고 있어 무한으로 발산하거나 0으로 수렴하는 경향 때문에 많은 sequence를 진행하지 못하는 단점을 보완하고 있습니다. 바로 직전의 cell state 정보를 넘겨줌으로서 sequence가 길어져도 gradient가 소실되는 것을 방지할 수 있습니다.   

Cell state와 hidden 벡터를 모두 계산하기 위해 cell 내부에는 4가지의 gate가 존재합니다.   
이 4개의 gate는 입력 벡터와 hidden 벡터 그리고 weight에 의해 생성됩니다.   
![ifog](/img/LSTMifog.png)
> i: Input gate, whether to write to cell
> f: Forget gate, whether to erase cell
> o: Output gate, how much to reveal cell
> g: Gate gate, how much to write to cell

![LSTM](/img/LSTM.png)

위 도식을 식으로 나타내면 다음과 같습니다.  
> $f_t = \sigma(W_f \times[h_{t-1},x_t] + b_f)$
> $i_t = \sigma(W_i \times[h_{t-1},x_t] + b_i)$
> $\tilde{C_t} = tanh(W_c \times[h_{t-1},x_t] + b_C)$
> $C_t = f_t \times C_{t-1} + i_t\times \tilde{C_t}$
> $o_t = \sigma(W_o[h_{t-1},x_t] + b_o)$
> $h_t = o_t \times \tanh(C_t)$

<br>

### GRU
GRU는 LSTM에서 cell state를 나타내는 $C_t$를 hidden 벡터인 $h_t$에 포함하도록하여 계산량을 줄이도록 한 RNN 계열의 모델입니다. 이와 더불어 아래 도식을 보면 LSTM에서 Forget gate와 Input gate를 사용하여 다음 cell state를 계산한 것처럼 GRU에서는 Input gate인 $z_t$와 Forget gate의 역할을 대신하는 $(1-z_t)$를 활용하여 다음 cell state, GRU에서는 hidden 벡터 $h_t$를 계산하게 됩니다.   

![GRU](/img/GRU.png)

<br> 

## 피어 세션
---
금일은 마스터 세션에서 공통적으로 질문할 수 있을거리에 대해 이야기를 나누었습니다. 이번 주차는 NLP를 주제로 공부하고 있다보니 이와 관련된 커리어에 대한 이야기를 많이 나누게 되었습니다. 또한, 한국어가 언어적으로 어렵다는 판단이 들어 영어와 비교하여 어떤 장단점이 있는지에 대해서도 이야기해 보았습니다.   
이와 더불어 dacon 대회에 쓸 모델로 EfficientNet을 한정된 시간으로 colab에서 학습시키기 위해 checkpoint를 설정하여 이어서 학습을 진행할 수 있는 방법에 대해 알아보았습니다. 
