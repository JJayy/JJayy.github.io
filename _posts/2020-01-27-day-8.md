---
layout: post
title: Day8
hide_title: False
excerpt: Boost Camp AI Tech - DAY 8
toc: true
toc_sticky: true
# toc_label: Category
feature-img: assets/img/feature-img/story.jpeg
author: Jay
tags: [Boost camp, AI tech, Day8]
---

# Day 7

#### 진행 사항
  - [x] pandas 1
  - [x] Mathematics for Artificial Intelligence
    - [x] 5강: 딥러닝 학습방법 이해하기
  - [x] 피어세션 
  - [x] 개인학습


## 학습 내용
---
금  

다음은 강의 내용을 정리한 것입니다.   

<br> 

## Pandas 1
---

### pandas
panel data의 줄임말로 구조화된 데이터의 처리를 지원하는 Python 라이브러리입니다. 고성능 array 계산 라이브러리인 numpy의 기능들을 지원하면서 강력한 '스프레드시트'처리 기능을 제공합니다. 인덱싱, 연산용 함수, 전처리 함수 등을 제공합니다. 파이썬계의 엑셀이라고 생각할 수 있으며, 테이블 형태 데이터를 다루는데에 특화되어 있어 데이터 처리 및 통계 분석을 위해 사용합니다.   
~~~python
import pandas as pd

data_url = 'csv파일 주소'
df_data = pd.read_csv(data_url, sep='\s+', header = None)  # csv 타입 데이터 로드, seperate는 빈공간(스페이스)로 지정하고 column은 없음
df_data.head()  # 데이터의 첫 5줄 출력 (default가 5줄)

df_data.columns = ['이름', '생년월일', '나이']  # column header 이름 지정
~~~

#### Series & dataframe
pandas 자료의 구성은 데이터 테이블 전제를 포함하는 오브젝트인 dataframe과 각 column에 해당하는 데이터의 모음 object인 series가 있습니다. 즉, dataframe은 series의 모음이기도 합니다.      
일반적으로는 데이터를 불러와서 쓰기 때문에 dataframe을 따로 생성할 일은 없으나 직접 데이터를 가지고 데이터 테이블을 만들기 위해서는 dataframe 함수를 적용해야 합니다.   

**Series**   
시리즈 함수는 다음 인자를 바탕으로 생성됩니다.   
> Series(data=None, index=None, dtype=None, name=None, copy=False, fastpath=False)

먼저 Series를 생성하는 방법에 대해 알아보겠습니다.   

~~~python
from pandas import Series
import pandas as pd
import numpy as np

list_data = [1,2,3]
example_obj = Series(data=list_data)
example_obj
>>> 0  1
>>> 1  2
>>> 2  3
>>> dtype: int64

index_data_1 = ['a', 'b', 'c', 'd']
index_data_2 = ['e', 'f']
example_obj_2 = Series(data=list_data, index = index_data_1)
example_obj_2
>>> a  1
>>> b  2
>>> c  3
>>> d  NaN   # index 기준으로 column 생성
>>> dtype: int64

example_obj_3 = Series(data=list_data, index = index_data_2)
>>> e  1
>>> f  2  # index 기준으로  column 생성
>>> dtype: int64


example_obj_3.index   # 인덱스 리스트
>>> Index(['e', 'f'], dtype='object')
example_obj_3.values
>>> array([1, 2])   # 값 리스트
type(example_obj_3.values)
>>> numpy.ndarray


dict_data = {"a":1, "c": 2, "e": 3}
example_obj_4 = Series(dict_data, dtype=np.float32, name="example_data")
example_obj_4
>>> a  1.0
>>> c  2.0
>>> e  3.0
>>> Name: example_data, dtype: float32
~~~

다음으로는 Series를 다루는 방법에 대해 알아보겠습니다.   

~~~python
dict_data = {"a":1, "c": 2, "e": 3}
example_obj = Series(dict_data, dtype=np.float32, name="example_data")

example_obj["a"] # 인덱스를 통해 값 불러오기
>>> 1.0
example_obj["a"] = 4.0 # 값 바꾸기
example_obj
>>> a  4.0
>>> c  2.0
>>> e  3.0
>>> Name: example_data, dtype: float32

example_obj = example_object.astype(int) # 데이터 타입 바꾸기
example_obj
>>> a  4
>>> c  2
>>> e  3
>>> Name: example_data, dtype: int64

example_obj.name = "number"  # Series 이름 지정
example_obj.index.name = "alphabet"  # index 열의 이름 지정
~~~


**Dataframe**   
Series들이 모여 만들어진 것이 dataframe입니다. 때문에 기본적으로 2차원 행렬이게 됩니다. dataframe 함수의 인자는 다음과 같습니다.   
> DataFrame(data=None, index=None, columns=None, dtype=None, copy=False)

~~~python
from pandas import Series, DataFrame
import pandas as pd
import numpy as np

raw_data = {'first name':['Jason','Amy'], 'last name':['Miler', 'Ali'], 'age':[42, 24]}
df = pd.DataFrame(raw_data, columns = ['first_name', 'last_name', 'age'])  # raw_data의 모든 값 불러오기
                            # columns 에서 가져오고 싶은 값만 선택하거나 아예 새로운 값을 넣어 열(column)을 추가할 수 있음
                            # 이때 값은 NaN으로 채워지게 됨
df.first_name  # column 이름이 first_name인 series를 추출
df["first_name"]  # column 이름이 first_name인 series를 추출
~~~

index를 통해 데이터를 처리하는 방법에는 loc과 iloc이 있습니다. loc은 index의 이름으로 호출하며, iloc은 index의 번호로 호출합니다.   
~~~python
s = pd.Series(np.nan, index=[11, 13, 0, 2, 1])
s.loc[:2]
>>> 11  NaN
>>> 13  NaN
>>> 0   NaN
>>> 2   NaN

s.iloc[:2]
>>> 11  NaN
>>> 13  NaN
~~~


##### selection & drop



#### dataframe operations



#### lambda, map, apply



#### pandas built-in functions












<br>

## Mathematics for Artificial Intelligence
---   
**딥러닝 학습 방법 이해하기**    

앞서 데이터를 선형모델로 해석하여 목적식인 y를 구하기 위해 y와 가장 근사하는 y^(y hat)을 만들어주는 beta 값을 경사하강법을 통해 학습하는 내용에 대해 배웠습니다. 복잡한 패턴을 가진 모델의 경우 선형모델만드로는 예측이 어렵게 됩니다. 때문에 신경망을 모델에 고려하게 되는데 이와 같은 신경망은 비선형모델입니다. 그러나 신경망 내부를 들여다보면 선형모델과 비선형모델의 결합으로 이루어져 있음을 알 수 있습니다. 신경망을 수식으로 분해하여 선형모델을 기반으로 비선형모델의 패턴을 학습할 수 있는지에 대해 알아보겠습니다.   

### 신경망(neural network)
신경망은 선형 모델과 비선형 모델인 활성함수(activation fuction)를 합성한 함수입니다. 이와 같은 신경망이 여러층 합성 된 함수를 다층(multi-layer) 퍼셉트론(MLP)이라고 부릅니다. 일반적으로 층이 깊을수록 목적함수를 근사하는데 필요한 뉴런(노드)의 숫자가 훨씬 빨리 줄어들기 때문에 효율적으로 학습이 가능합니다.    

#### 선형 모델
우리가 가진 데이터 셋 행렬을 X(n x d), 가중치 행렬을 W(d x p) 그리고 절편 벡터 b(n x p)라고 할때 출력값인 행벡터는 O(n x p)로 표현할 수 있습니다.   

> O = X * W + b

이때 가중치(weight)의 차원(p)에 따라 기존 차원(d)에서 p로 바뀌게 됩니다. 즉, d개의 변수로 p개의 선형모델을 만들어서 p개의 잠재변수를 설명하는 모델을 상상해 볼 수 있습니다. 

#### 활성함수
활성함수에 대해 이야기하기 전에 softmax 함수에 대해 알아보겠습니다.   

**softmax()**   
softmax는 앞서 구한 선형 모델의 출력 벡터를 합성하여 특정 클래스 k에 속할 확률벡터를 만들어 주는 역할을 합니다. 때문에 분류 문제를 풀 때 선형모델과 softmax() 함수를 결합하여 예측합니다.   
> softmax(O) = softmax(Wx+b)

~~~python
def softmax(vec):
    denumerator = np.exp(vec - np.max(vec, axis=-1, keepdims=True))
    numerator = np.sum(denumerator, axis=-1, keepdims=True)
    val = denumerator/numerator
    return val

vec = np.array([[1,2,0],[-1,0,1],[-10,0,10]])
softmax(vec)
>>> array([[2.44728471e-01, 6.65240956e-01, 9.00305732e-02],
           [9.00305732e-02, 2.44728571e-01, 6.65240956e-01],
           [2.06106005e-09, 4.53978686e-05, 9.99954500e-01]])
~~~
확률에 대한 학습을 진행할 떄는 softmax() 함수를 이용하지만, 결과값을 추론할 때는 one-hot 벡터로 최대값을 가진 주소만 1을 출력하는 연산을 softmax()를 사용하지 않고 직접 출력 벡터에 적용합니다.   
~~~python
def on_hot(val, dim):
    return [np.eye(dim)[_] for _ in val]

def one_hot_encoding(vvec):
    vec_dim = vec.shape[1]
    vec_argmax = np.argmax(vec, axis=-1)
    return one_hot(vec_argmax, vec_dim)

def softmax(vec):
    denumerator = np.exp(vec - np.max(vec, axis=-1, keepdims=True))
    numerator = np.sum(denumerator, axis=-1, keepdims=True)
    val = denumerator/numerator
    return val

vec = np.array([[1,2,0],[-1,0,1],[-10,0,10]])
one_hot_encoding(vec)
>>> array([[0.,1.,0.], [0.,0.,1.],[0.,0.,1.]])
one_hot_encoding(softmax(vec))
>>> array([[0.,1.,0.], [0.,0.,1.],[0.,0.,1.]]) #softmax한 값도 같은 값을 가지므로 굳이 softmax를 연산할 필요가 없다.
~~~

**활성 함수**   
앞서 이야기한 softmax의 활성함수 트릭을 이용하여 선형모델을 활성함수를 통해 비선형함수로 만들 수 있습니다. 활성함수 자체는 비선형 함수로 앞서 선형 모델의 출력벡터(잠재벡터)의 각 노드에 개별적으로 계산을 진행하여 새로운 잠재벡터(H)를 만듭니다. softmax()와의 차이점이라면 softmax는 전체 데이터와 한꺼번에 결합하여 계산하지만, 활성함수는 각 노드에 개별적으로 적용된다는 차이점이 있습니다. (활성함수는 소문자 시그마 기호를 씁니다.)   

> 잠재벡터(z) = Wx + b
> H = sigma(z) = (sigma(z1), sigma(z2), ... , sigma(zn)) = sigma(Wx+b)

활성함수는 비선형(nonlinear)함수로 이를 적용하지 않으면 딥러닝은 선형모양과 차이점이 없게 됩니다. 시그모이드(sigmoid) 함수나 tanh 함수가 전통적으로 많이 쓰이지만 딥러닝에서는 ReLU 함수를 많이 쓰고 있습니다.   

> ReLu(x) = max{0, x}

#### 다층 신경망
선형 모델의 잠재벡터(z)에 활성함수를 적용하여 얻은 잠재벡터(H)에 다시 가중치 행렬(W2)와 절편 벡터(beta2)를 통해 다시 선형 변환을 진행할 수 있으며, 이렇게 구성한 신경망을 W2, W1을 패러미터로 가진 2층(2-layers) 신경망이라고 합니다.   

2개 이상의 층을 가진 신경망을 다층 신경망이라고 부르며, 수식으로 나타낼 때 L개의 가중치 행렬로 이루어져 있다고 표현합니다.   
> O = Z_l
> ...
> H_l = sigma(Z_l)
> Z_l = H_l-1 *W_l + beta_l
> ...
> H_1 = sigma(Z_1)
> Z_1 = XW_1 + beta_1

이렇게 다층 신경망을 이용하는 이유는 이론적으로 2개 이상의 층을 가지게 되면 임의의 연속함수를 근사할 수 있기 때문입니다. 이를 universal approximation theorem이라고 합니다. 층이 깊을수록(많을수록) 선형 변환에 의해 d차원에서 p 차원으로 (d>p) 변화하기 때문에 필요한 뉴런(노드)의 숫자가 훨씬 빨리 줄어들어 조금 더 효율적인 학습이 가능합니다.    

#### 역전파 알고리즘
지금까지 신경망을 구성하는 과정은 순서대로 진행되는 forwardpropagation이라고 합니다. 하지만 여기서 우리는 가중치 행렬(W)에 대해 학습하는 것이 목적이기 때문에 이 과정을 꺼꾸로 진행하는 역전파(backpropagation) 알고리즘을 이용하여 각 층에 사용된 패러미터 W와 beta를 학습합니다.   

각 층의 패러미터의 그레디언트 벡터는 윗층부터 역순으로 계산하게 됩니다. 이 과정에서 L부터 1의 순서로 연쇄법칙을 통해 그레디어트 벡터를 전달합니다.   
> dL/dW_l = dL/dO  x  ...  x dZ_l/dW_l
> dL/dbeta_l = dL/dO x ... x dZ_l/dbeta_l



<br>

## 피어세션
---
dacon 참여 논의

softmax one hat

max 빼는 이유

비선형 이유(선형으로 풀 수 없는 XOR 문제)

span

<br>

## 개인 학습
---
앞서