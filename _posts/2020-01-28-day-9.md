---
layout: post
title: Day9
hide_title: False
excerpt: Boost Camp AI Tech - DAY 9
toc: true
toc_sticky: true
# toc_label: Category
feature-img: assets/img/feature-img/story.jpeg
author: Jay
tags: [Boost camp, AI tech, Day9]
---

# Day 9

#### 진행 사항
  - [x] pandas 2
  - [x] Mathematics for Artificial Intelligence
    - [x] 6강: 확률론 맛보기
  - [x] 피어세션 
  - [x] 개인학습


## 학습 내용
---
금일은 지난 시간에 이어 pandas를 사용하는 방법과 통계학의 기초가 되는 확률론에 대해 공부하였습니다.   

다음은 강의 내용을 정리한 것입니다.   

<br> 

## Pandas 2
---
지난시간에 이어 pandas의 함수들을 다루는 방법에 대해 공부하였습니다.   

### Groupby
특정 기준에 따라 데이터를 묶어주는 함수입니다.   
> df.groupby("묶음의 기준이되는 column name")["적용받는 column"].연산(sum(), mul(), 등)

묶는 기준이 되는 column의 같은 값을을 묶에 적용받는 column의 값들을 연산하여 하나로 묶어 보여줍니다.    
ex) 연도별 스포츠팀 승점   
groupby("팀")["승점"].sum() >>> 각 팀의 총점을 더하여 팀별로 나열(연도별 팀별 승점 데이터를 팀별 총승점으로 group화)    

여러개의 column을 묶음의 기준으로 설정할 수 있습니다. 이때 index의 개수가 묶는 column 개수 만큼 생기게 되는데, index level은 먼저오는 순으로 설정됩니다.   

Group으로 묶인 데이터를 matrix 형태로 전환하기 위해서는 .unstack()이라는 함수를 쓸 수 있으며, .swaplevel()을 이용하여 index level을 바꾸거나 .sortlevel(), .연산(level="level index number")를 통해 level에 따라 정렬하거나 연산을 할 수 있습니다.   

이 외에도 groupby("묶음의 기준이되는 column name") 만 사용하면 Tuple형태로 group의 key 값과 value 값이 추출되어 이를 unpacking하여 split하게 추출할 수 있습니다.   
또한, .get_group("column name")을 통해 그룹에서 특정 key 값을 가진 그룹의 정보만 추출이 가능합니다.   

이렇게 추출한 group 정보에는 3가지 유형의 apply가 가능합니다.   
- aggregation : 요약된 통계정보를 추출
- transformation : 해당 정보를 변환
- filtration : 특정 정보를 제거하여 보여줌
  
**aggregation**
.agg()로 사용하며 괄호 안에는 원하는 연산 함수를 넣어 통계정보를 얻을 수 있습니다.   
ex) .agg(sum), .agg(np.mean), .agg([np.sum, np.mean, np.std]) 등    

**transformation**   
aggregation과 달리 key값 별로 요약된 정보가 아닌, 개별 데이터의 변환을 지원합니다.   
ex) .transform(lambda x: (x))    
다만 x.min()이나 x.max()와 같은 series 데이터에 적용되는 데이터들은 key값을 기준으로 grouped된 데이터 기준입니다.   

**filter**   
.filter()로 사용하여 괄화 안에 boolean 조건을 넣어 데이터를 검색합니다.   
ex) df.groupby('Team').filter(lambda x: x["points"].sum() > 100)   


### Pivot table Crosstab
Excel의 pivot table과 같은 기능을 제공합니다. index 축은 groupby와 동일하며 column에 추가로 라벨링 값을 추가하여 값에 numeric type 값을 aggregation 하는 형태 입니다.   

~~~python
 # 통화량 데이터
 import pandas as pd
 import numpy as np
 import dateutil

df_phone = pd.read_csv("phone_data.csv")
df_phone['date'] = df_phone['date'].apply(dateutil.parser.parse, dayfirst=True)

df_phone.pivot_table(["duration"],
                     index=[df_phone.month, df_phone.item],
                     colums=df_phone.network, aggfunc="sum", fill_value=0)
 #month가 최상위 index, item이 하위 index로 들어가고, network가 column이 되어
 #해당하는 값들을 sum 하여 pivot table로 표시합니다.
 #fillvalue가 0이므로 데이터가 1개여도 NaN으로 표시되지 않고 0과의 덧셈으로 표시됩니다. 
~~~

**crosstab**   
두 column에 교차 빈도, 비율, 덧셈 등을 구할 때 사용하는 pivot table의 특수한 형태입니다. user-item rating matrix와 같은 것을 만들 때 활용 가능합니다.   

~~~python
df_movie = pd.read_csv("./movie_rating.csv")

pd.crosstab(["rating"],
            index=df_movie.critic,
            colums=df_movie.title, 
            values=df_movie.rating, aggfunc="first").fillna(0)
 #critic이 인덱스가 되며, title이 새로운 column이 됩니다. 
 #fillna(0)은 fill_value=0과 같습니다. 
~~~

### Merge & Concat
merge는 두개의 데이터를 하나로 합치는 함수이고, concat는 같은 형태의 데이터를 붙이는 함수힙니다.   

**merge**   
>pd.merge(df_a, df_b, on='기준')
>pd.merge(df_a, df_b, left_on='a기준', right_on='b기준') #합치려고하는 두 dataframe의 column이름이 다를때

~~~python
 #join method
 #inner, outer, left, right가 존재
 #inner는 교집합, outer는 합집합, left는 왼쪽 집합만, right는 오른쪽 집함만을 의미합니다. 
pd.merge(df_a, df_b, on='기준 column name', how='조인방법')
 
 #index number로 join
pd.merge(df_a, df_b, right_index=True, left_index=True)
 #index이름이 없다면 인덱스 넘버를 가르키기 때문에 같은 인덱스 넘버끼리 합치게 됩니다.
~~~

**concat**   
colum과 index 형식이 같은 데이터를 이어 붙일 때 사용합니다.   
~~~python
df_new = pd.concat([df_a, df_b], axis='0') # 0이면 아래로 붙이고 1이면 위로 붙임
df_new.reset_index()

 #또는 

df_a.append(df_b)
~~~

### persistence
data loading시 db connection 기능을 제공합니다.   
~~~python
 #database 연결 코드
import sqlite3
conn = sqlite3.connect("./data/flights.db")
cur = conn.cursor()
cur.execute("select * from airlines limit 5;")
results = cur.fetchall()
results

#db 연결 conn을 사용하여 dataframe 생성
df_airplanes = pd.read_sql_query("select * from airlines;", conn)
df_airports = pd.read_sql_query("select * from airports;", conn)
df_routes = pd.read_sql_query("select * from routes;", conn)
~~~

**XLS persistence**   
datafrome의 엑셀 추출 코드로 xls엔진으로 openpyxls 또는 xlsxwirte 사용 가능합니다.   
~~~python
writer = pd.ExcelWriter('./data/df_routes.xlsx', engine='xlsxwriter')
df_routes.to_excel(writer, sheet_name='Sheet1')
~~~

**Pickle persistence**   
가장 일반적인 python 파일 persistence로 to_pickle, read_pickle 등의 함수를 사용합니다.   
~~~python
df_routes.to_pickle("./data/df_routes.pickle")

df_routes_pickle = pd.read_pickle("./data/df_routes.pickle")
~~~


<br>

## Mathematics for Artificial Intelligence
---   
### 확률론 

딥러닝은 확률론 기반의 기계학습 이론에 바탕을 두고 있습니다. 기계학습에서 사용되는 손실함수(loss function)들의 작동 원리는 데이터 공간을 통계적으로 해석해서 유도하게 됩니다. 특히 회귀 분석에서 손실함수로 L2-norm을 사용하는데, 이 예측오차의 분산을 가장 최소화하는 방향으로 학습하도록 유도합니다.   
또한 앞으로 공부할 교차엔트로피(cross-entropy)에서도 분류 문제의 모델 예측의 불확실성을 최소화하는 방향으로 학습하도록 유도합니다.   
이처럼 분산 및 불확실성을 최소화하기 위해서 이를 측정하는 방법에 대해 알아하고, 이는 통계학과도 이어지기 때문에 기계학습을 이해하려면 확률론의 기본 개념이 필요합니다.   

#### 확률분표
데이터 공간을 $x$ x $y$ 라고 표기하고 이 데이터 공간에서 데이터를 추출하는 분표를 $D$라고 표현합니다. 이러한 데이터는 확률변수로 (x, $y$) ~ $D$라고 표기하며, (x, $y$) $\in$ $x$ x $y$는 데이터 공산 상의 관측 가능한 데이터에 해당합니다.   


#### 이산확률 변수 & 연속확률 변수
확률 변수는 확률분표$D$에 따라 이산형(discrete)과 연속형(continuous) 확률변수로 구분하게 됩니다.   

이산형 확률변수는 비연속적이기 때문에 확률변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 더해서 모델링하고, 연속형 확률변수는 데이터 공간에 정의된 확률변수의 밀도(density) 위에서 적분을 통해 모델링을 합니다.    
밀도는 누적확률분포의 변화율을 모델링하는 것이기 때문에 확률 자체로 해석하는 오류를 범하지 않아야합니다.   

**결합 분포**   
확률분포 $D$는 결합분포 P(x,$y$)로부터 모델링합니다. P(x)는 입력 x에 대한 주변확률분포로 y에 대한 정보를 주지는 않습니다. 주변확률분포(P(x))는 결합분포(P(x, $y$))로부터 아래와 같이 유도 가능합니다.    
> 이산확률분포 : P(x) =  $\sum_y   P($x, $y)$
> 연속확률분포 : P(x) =  $\int_y P($x, $y)dy$ 


#### 조건부확률과 기계학습
조건부확률분포 P(x|$y$)는 데이터 공간에서 입력 x와 출력 $y$사이의 관계를 모델링하는데, 이는 $y$가 주어졌을 때 x에 대한 조건분포를 의미합니다. 즉, 특정 클래스($y$)가 주어진 조건에서 데이터(x)의 확률분포를 보여줍니다.    
이때 이산확률분표의 경우 P(x|$y$)는 확률 자체를 의미하지만, 연속확률분포에서는 밀도(density)로 해석합니다.   

**기계학습**    
지난 시간에 softmax()를 로지스틱 회귀에서 사용했던 선형모델을 결합시키면 데이터에서 추출된 패턴을 기반으로 확률을 나타내준다는 것을 배웠습니다. 이를 바탕으로 분류 문제에서 softmax(W$\phi$ + b)는 데이터 x로부터 추출된 특징패턴 $\phi$(x)과 가중치행렬 W를 통해 조건부확률 P($y$|x)를 계산합니다.   
여기서 P($y$|x)는 데이터 x가 주어졌을 때 특정 클래스 $y$의 확률분포를 나타내는 것으로 전체 데이터 x 대신 $\phi$(x)로 써도 무방합니다.   

회귀문제의 경우 앞서 계산한 조건부확률을 통해 조건부 기대값 E[$y$\|x]을 추정합니다.   
> $E_{y - P(y|x)} [y|x] =  \int_y yP(y|x)dy$


여기서 조건부 기대값인 E[$y$\|x]은 E$||y-f(x)||_{2}$을 최소화하는 함수 $f(x)$와 일치합니다.   

**기대값**   
이처럼 확률분포가 주어지면 데이터를 분석하는데 사용 가능한 여러 종류의 통계적 범함수(statistical functional)를 계산할 수 있습니다.   
기대값(expectation)은 데이터를 대표하는 통계랑이면서 동시에 확률분포를 통해 다른 통계적 범함수를 계산하는데 사용됩니다. 이산확률분포는 급수를, 연속확률분포는 적분을 사용하여 계산합니다.    
> $E_{x - P(x)} [f(x)] =  \int_X f(x)P(x)dx$
> $E_{x - P(x)} [f(x)] = \sum_{x \in X} f(x)P(x)$

이러한 기대값을 이용하여 분산, 첨도, 공분산 등 여러 통계량을 계산할 수 있습니다.   


결과적으로 딥러닝은 다층신경망을 사용하여 데이터로부터 특징패턴 $\phi$을 추출합니다.   


#### 몬테카를로 샘플링(Monte Carlo Sampling)
기계학습의 많은 문제들은 확률분포를 명시적으로 모를 때가 대부분이기 때문에, 데이터를 이용하여 기대값을 계산할 수밖에 없습니다. 이렇게 계산을 하기 위해서는 몬테카를로(Monte Carlo) 샘플링 방법을 사용합니다.    
> $E_{x - P(x)} [f(x)] =$ $\frac{1}{N} \sum_{i=1}^N f(x^{(i)}),$  $x^{(i)} - (i.i.d.) P(x)$

몬테카를로 샘플링에서는 이산확률과 연속확률에 상관없이 독립추출만 보장된다면 대수의 법칙(law of large number)에 의해 수렴성을 보장합니다.  


<br>

## 피어세션
---
이번 피어세션에서는 Dacon의 참가신청을 진행하였습니다. google colab을 이용한 baseline code가 기본 제공되어 이를 단순히 옮겨 실행해보는 것을 목표로 진행하였습니다.   
이를 위해 기본적인 google colab 이용 방법과 google drive를 사용하기 위한 권환 획득과 path 설정 방법에 대해 알아보았습니다.   
주어진 baseline code에서는 5개의 fold와 10개의 epoch를 사용하여 best model 값을 리턴하도록 설계가 되어있었고, 약 2-3시간 동안 colab을 활용하여 주어진 모델 학습을 진행하였습니다.   
아직 코드가 어떻게 설계되었는지, 또 어떻게 수정해야 성능을 높힐 수 있는지에 대한 것이 부족하여 앞으로 피어세션을 통해 계속해서 공부할 예정입니다. 이를 위해 조원 모두 pytorch를 사용하여 model을 구현하도록 통일할 예정입니다.   

<br>

## 개인 학습
---
피어세션에 진행했던 dacon 대회에 필요한 기초 지식들을 찾아보았습니다. baseline으로 제시한 google colab 사용법부터, pytorch, 그리고 학습모델로 사용한 resnet에 대해 간략히 찾아보았습니다.   
Latex 문법이 블로그에 적용되지 않아 post.html을 수정하는 방식을 찾았지만 실패하였습니다. 다르게 변경하는 방법에 대해 알아보려고 합니다.   