---
layout: post
title: Day11
hide_title: False
excerpt: Boost Camp AI Tech - DAY 11
use_math: true
toc: true
toc_sticky: true
# toc_label: Category
feature-img: assets/img/feature-img/story.jpeg
author: Jay
tags: [Boost camp, AI tech, Day11]
---

# Day 11

## 학습 내용
  - [x] PyTorch
  - [x] Mathematics for Artificial Intelligence
    - [x] 8강: 베이즈 통계학 맛보기
  - [x] DL Basic
    - [x] 딥러닝
    - [x] 뉴럴 네트워크 - MLP

<br>

## 개인 학습
---
### 베이즈 정리 (Bayes' Theorem)
베이즈 정리는 조건부확률을 이용하여 두 확률 변수의 사전 확률과 사후 확률 사이의 관계를 나타내는 정리입니다. 사전 확률로부터 사후 확률을 구할 수 있다는 점에서 불확실성 하에 의사결정문제를 수학적으로 다룰 떄 중요하게 이용됩니다. 때문에 베이즈 정리는 딥러닝에 있어 정보를 갱신하는데에 중요하게 사용됩니다.

> $P( \theta  | D) = P( \theta ) \frac{P(D |  \theta) }{P(D)}$

식은 위와 같이 일반적인 조건부확률을 따르며 $P( \theta )$는 사전 확률(Prior), $P( \theta  | D)$는 사후 확률(Posterior)를 나타냅니다. $\frac{P(D |  \theta) }{P(D)}$에서 가능도(Likelihood) $P(D|\theta)$와 Evidence $P(D)$를 통해 사전 확률로부터 사후 확률을 구하고, 그 사후 확률을 다시 사전 확률로 사용하는 방식으로 정보를 갱신할 수 있습니다.    

같은 방식으로 사전확률, 민감도(Recall), 오탐율(False alarm)을 가지고 정밀도(Precision)을 계산할 수 있습니다. 
> $P( \theta  | D) = P( \theta ) \frac{P(D |  \theta) }{P(D)}$
> $P(D) = \sum_\theta P(D|\theta)P(\theta) = P(D|\theta) * P(\theta) + P(D|\neg\theta)*P(\neg\theta)$

여기서 $P( \theta)$는 사전 확률, $P(D|\theta)$는 민감도를 의미하여 이 둘의 곱은 True Positive로 정의됩니다. 이에 반대되는 항인 $P(D|\neg\theta)*P(\neg\theta)$는 오탐율과 사전 확률의 반대 확률의 곱으로 False Positive(1종 오류)로 정의됩니다. 이 외에도 False Negative(2종 오류)나 True Negative도 존재하지만, 베이즈 정리의 민감도 즉 사후 확률은 위 식으로부터 $\frac{True Positive}{True Positive + False Positive}$로 표현 되기 때문에 False Positive를 줄여 정밀도를 높이게 됩니다.

<br>

### Deep Learning Basic
#### 딥러닝이란
딥러닝은 크게 Data, Model, Loss, Algorithm 이 4가지 구성을 통해 이해할 수 있습니다. 그 중에서도 Loss function과 Optimization algorithm에 대해 알아보겠습니다.   
먼저 Loss Function은 딥러닝을 통해 학습하는 weight와 bias의 밯향성을 제시해주는 역할을 합니다. 딥러닝을 통해 학습한 결과와 실제 얻고자 하는 데이터 간의 차이를 계산한다고 생각할 수 있습니다. 다음과 같은 loss function들이 일반적으로 쓰이고 있습니다.   
> Regression Task : $MSE = \frac{1}{N} \sum^N_{i=1} \sum^D_{d=1} (y_i^{(d)} - \hat{y}_i^{(d)})^2$
> Classification Task : $CE = -\frac{1}{N} \sum^N_{i=1} \sum^D_{d=1} y_i^{(d)} log y_i^{(d)}$
> Probabilistic Task : $MLE = \frac{1}{N} \sum^N_{i=1} \sum^D_{d=1} log N(y_i^{(d)} ; y_i^{(d)} , 1)$ (= MSE)

Optimization, 최적화는 data, model, loss function이 정의되어 있을 때 네트워크를 어떻게 줄일지에 대해 정의한 것입니다. SGD, Momentum, Adam 등 여러 optimizer들이 있으며, 이 외에도 dropout, k-fold validation, batch normalization 등의 테크닉을 함께 사용하여 네트워크의 성능을 향상시키도록 설계합니다.    

<br>

## 피어 세션
---
금일은 지난 주말동안 수행한 개인 학습에 대해 이야기를 나누었습니다. 강의 내용 자체에서는 크게 어려울 것이 없었으나 Pytorch를 처음 다루는데 있어 익숙치 않은 조원들이 많았습니다. 때문에 피어세션에서 Youtube에 있는 Pytorch 강의를 추천받아 단기간 내에 더 익숙해질 수 있도록 정보를 공유하였습니다. 