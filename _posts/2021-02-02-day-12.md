---
layout: post
title: Day12
hide_title: False
excerpt: Boost Camp AI Tech - DAY 12
use_math: true
toc: true
toc_sticky: true
# toc_label: Category
feature-img: assets/img/feature-img/story.jpeg
author: Jay
tags: [Boost camp, AI tech, Day12]
---

# Day 12

## 학습 내용
  - [x] Mathematics for Artificial Intelligence
    - [x] 9강: CNN 첫걸음
  - [x] DL Basic
    - [x] Optimization

<br> 

## 개인 학습
---
### CNN
#### Convolution
Convolution 연산은 커널(kernel)을 입력 벡터 상에서 움직여가며 선형 모델과 합성 함수가 적용되는 구조입니다.   
> 기존 MLP : $h_i = \sigma (\sum_{j=1}^p W_{ij}x_j )$
> Convolution : $h_i = \sigma (\sum_{j=1}^k V_jx_{i+j-1})$
> 모든 $i$에 대해 적용되는 커널은 $V$로 같고 커널 사이즈 $k$만큼 $x$상에서 이동하면서 적용

위 식처럼 활성화 함수를 제외한 Convolution 연산도 선형변환에 속합니다.   
Convolution 연산을 수학적인 의미로 해석하면, 신호(signal)을 커널을 사용하여 국소적으로 증폭 또는 감소시켜 정보를 추출 또는 필터링 하는 것입니다.   

> continuous : $[f*g](x) = \int_{R^d} f(z)g(x \pm z)dz = \int_{R^d} f(x \pm z)g(z)dz = [g*f](x)$
> discrete : $[f*g](i) = \sum_{a \in Z^d} f(a)g(i \pm a) = \sum_{a \in Z^d} f(i \pm a)g(a) = [g*f](i)$

커널은 정의역 내에서 움직여도 변하지 않는 translation invariant하며 주어진 신호에 국소적(local)으로 적용합니다. 이와 같은 특징으로 Convolution 연산은 1차원뿐만 아니라 다양한 각 차원에서 커널을 일정하게 유지하며 계산 가능합니다.   

특히 matrix나 tensor를 자주 다루는 딥러닝의 특성상 2차원 Convolution이 가장 많이 쓰이게 되며, 식으로 나타내면 다음과 같습니다. 
> $[f*g](i,j) = \sum_{p,q} f(p,q)g(i+p, j+q)$  
> $f$는 커널, $g$는 입력벡터를 의미

입력 크기를 $(H, W)$, 커널 크기를 $(K_H, K_W)$, 출력 크기를 $(O_H, O_W)$라고 하면 출력 크기는 다음과 같습니다. 
>$O_H = H - K_H + 1$
>$O_W = W - K_W + 1$

#### Convolution 연산의 역전파
Convolution 연산의 역전파를 수식으로 나타내면 다음과 같습니다.   
> $\frac{\partial}{\partial x}[f*g](x) = \frac{\partial}{\partial x}\inf_{R^d} f(y)g(x-y)dy = [f*g'](x)$

위 수식에서 알 수 있듯이 convolution 연산은 커널이 모든 입력 데이터에 공통으로 적용되기 때문에 역전파를 계산할 떄도 convolution 연산이 나오게 됩니다. 
> $\frac{\partial L}{\partial w_i}[f*g](x) = \sum_j \partial_j x_{i+j-1}$

위 처럼 식이 유도되는데, 이는 각커널에 들어오는 모든 그레디언트를 더하는 것으로 결국 convolution 연산과 같다는 것을 알 수 있습니다. 

<br>

### Optimization
#### Concepts
  - **Generalization(일반화)**
   일반적으로 학습을 계속해서 진행하게 되면 Training error는 줄어들지만 unseen data 즉 새로운 데이터에 대한 성능이 떨어지는 경우가 있습니다. 이처럼 Test error와 Training error 사이의 차이를 Generalization gap이라고 부르며, training set 이외의 데이터에서도 성능을 보장하는 일반화(generalization)가 되어야 합니다.    
    <br>
  - **Underfitting & Overfitting**
   학습이 덜되어 성능이 떨어지는 경우를 underfitting, 학습이 너무 과도하게 많이 되어 필요 이상으로 fitting 되어 성능이 떨어지는 경우입니다.
   <br>
  - **Cross-validation**
   train data를 여러개의 파티션으로 나누어 한 파티션을 validation, 즉 test set으로 활용하고 나머지 파티션들을 모두 학습시켜 성능을 확인하는 기술입니다. 파티션 개수만큼 fold를 진행할 수 있으며, 매 학습에서 학습되지 않은 데이터로 validation을 평가하기 때문에 성능을 높일 수 있습니다.
   <br>
  - **Bias & Variance**
   Bais는 평균적으로 목적으로 하는 결과값에 근사하게 나오는지를 나타내며, 값이 낮을수록 원하는 결과값에 근사합니다. Variance는 결과값들이 비슷하게 출력되고 있는지를 확인합니다. 목적 결과와는 상관없이 결과값들이 비슷하게 형성되면 낮은 Variance를 갖습니다. 이 두 요소를 통해 원하는 목적 결과로 출력되도록 조절할 수 있습니다.
   <br>
  - **Bootstrapping**
   Booststrapping은 train data를 random sampling을 통해 각각의 모델을 학습시키고 각각의 모델이 예측하는 값들이 얼마나 일치를 이루는지를 활용하여 전체 모델의 assertant를 예측하고자 할 떄 사용합니다.
   <br>
  - **Bagging & Boosting**
   Bagging은 Bootstrapping aggregating의 약자로 앞서 설명한 Booststrapping처럼 train data를 random sampling을 통해 모델을 여러개 학습 시키고 한 결과 값에 대해 대다수의 모델이 일치하는 값을 결과값으로 사용하는 방식을 의미합니다. 이와 다르게 Boosting은 train data의 일부를 활용하여 모델을 만들고, 해당 모델의 성능이 떨어지는 data에 대해 해당 데이터만들 가지고 모델을 추가로 학습시킵니다. 이처럼 성능이 떨어지는 부분에 대해 모델들 새롭게 추가하는 방식으로 하나의 모델을 구성시키는 방법입니다.

#### Optimizers
**1. Stochastic Gradient Descent(SGD)**

**2. Momentum**

**3. Nesterov Accelerate**

**4. Adagrad**

**5. Adadelta**

**6. RMSprop**

**7. Adam**

   

#### Regularization
  - **Early Stopping**
       
    <br>
  - **Parameter Norm Penalty**
   
    <br>
  - **Data Augmentation**
   
    <br>
  - **Noise Robustness**
   
    <br>
  - **Label Smoothing**
   
    <br>
  - **Dropout**
   
    <br>
  - **Batch Normalization**
   

<br> 

## 피어 세션
---
금일은 Optimizer에 대해 이야기를 나누었습니다. Adam이 압도적인 성능을 보이고 있어 많은 경우에 있어 Adam을 optimizer로 사용하고 있다는 것을 알게 되었습니다. 
