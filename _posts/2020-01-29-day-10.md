---
layout: post
title: Day10
hide_title: False
excerpt: Boost Camp AI Tech - DAY 10
use_math: true
toc: true
toc_sticky: true
# toc_label: Category
feature-img: assets/img/feature-img/story.jpeg
author: Jay
tags: [Boost camp, AI tech, Day10]
---

# Day 10

#### 진행 사항
  - [x] 시각화 도구
  - [x] Mathematics for Artificial Intelligence
    - [x] 7강: 통계학 맛보기
  - [x] 피어세션 
  - [x] 개인학습


## 학습 내용
---
금 

다음은 강의 내용을 정리한 것입니다.   

<br> 

## 시각화 도구
---
지난 

### matplotlib


### matplotlib graph


### seaborn




<br>

## Mathematics for Artificial Intelligence
---   
### 통계학 맛보기
통계적 모델링은 적절한 가정 위에서 확률분포를 추정(inference)하는 것이 목표입니다. 일반적으로 유한한 개수의 데이터만 가지고는 모집단의 분포를 정확하게 알아내는 것이 불가능하므로 근사적으로 확률분포를 추정할 수밖에 없습니다.     

때문에 선험적(a priori)으로 데이터가 특정 확률분포를 따른다고 가정한 뒤에 그 분포를 결정하는 모수(parametric)를 추정하는 방법을 사용하게 되며, 이를 모수적(parametric) 방법론이라고 합니다.   
반면, 특정 확률분포를 가정하지 않고 데이터에 따라 모델의 구조 및 모수의 개수가 유연하게 바뀔 때는 비모수(nonparametric) 방법론이라고 부릅니다.    

기계학습의 경우, 대게 데이터에 따라 유동적이므로 비모수 방법론을 사용한 방법론을 많이 사용합니다.   

> 확률분표 가정 방법 *절대적인 방법은 아님
> - 데이터가 2개의 값(0 또는 1)만 가지는 경우  :  베르누이 분포
> - 데이터가 n개의 이산적인 값을 가지는 경우  :  카테고리 분포
> - 데이터가 [0,1] 사이에서 값을 가지는 경우  :  베타 분포
> - 데이터가 0 이상의 값을 가지는 경우  :  감마 분포, 로그 정규 분포 등
> - 데이터가 $R$ 전체에서 값을 가지는 경우  :  정규 분포, 라플라스 분포 등
>  
> 각 분포마다 각각의 검정 방법을 통해 추정한 분포를 검증해야합니다. 
> 또한, 위 가정 방법은 절대적인 것이 아니라 참고용이며, 데이터를 생성하는 원리를 고려하여 확률분포를 선택해야 합니다. 

이처럼 데이터의 확률분포를 가정했다면 모수(parametric)을 추정해 볼 수 있습니다.   

ex) 데이터의 확률분포가 정규분포일 때, 모수(parametric)인 평균 $\mu$과 분산 $\sigma^2$을 추정    
> $\mu$=$E[\bar{x}]$,  $\sigma^2$=$E[S^2]$
> $\bar{X} =  \frac{1}{N}  \sum_{i=1}^N X_i$  
> $S^2 =  \frac{1}{N-1}  \sum_{i=1}^N (X_i -  \bar{X})^2$   
> #표본분산에서 N이 아닌 N-1로 나누는 이유는 N-1로 나누어야 더 정확한 값을 가질 수 있기 때문이며
> #이 때 우리는 불편(unbiased) 추정량을 구한다고 말한다.

통계량의 확률분포를 표집분포(sampling distribution)라고 부르며, 특히 표본평균의 표집분포는 N이 커질수록 정규분포 $N(\mu, \sigma^2/N)$를 따릅니다. 이를 중심극한정리(Central Limit Theorem)이라고 부르며 모집단의 분포가 정규분포를 따르지 않아도 성립합니다.   


#### 최대가능도 추정법(MLE)
표본 평균이나 표본분산은 중요한 통계량이지만 확률분포마다 사용하는 모수가 다르기 때문에 적절한 통계량이 달라지게 됩니다. 최대가능도 추정법(maximum likelihood estimation, MLE)는 이론적으로 가장 가능성이 높은 모수를 추정하는 방법 중 하나입니다.   
> $\hat{\theta}_{MLE} = argmax_\theta L(\theta;x) = argmax_\theta P(x|\theta)$

여기서 가능도(likelihood) 함수란 모수 $\theta$를 따르는 분포가, x를 관찰할 가능성을 뜻하지만 확률적으로 해석하면 안됩니다. 쉽게 이해하기 위해, 확률은 확률분포에서 함수 아래 면적을, 가능성은 확률분포의 x값에 대한 y라고 생각할 수 있습니다.   

가능도 $L(\theta;X)$는 다음과 같이 표현됩니다.   
> $L(\theta;X) =  \prod_{i=1}^n P(x_i|\theta)$
> 여기서 데이터 집합 X가 독립적으로 추출되었을 경우 로그가능도로 변환이 가능합니다.(최적화)
> $log L(\theta;X) =  \sum_{i=1}^n log P(x_i|\theta)$

이처럼 $log$ 연산을 사용하면 곱셈을 덧셈 연산으로 바꿀 수 있는데, 이는 컴퓨터를 활용한 계산을 진행할 때 중요한 부분입니다. 데이터가 많아지면 컴퓨터에서는 data type에 따라 표현할 수 있는 숫자의 수가 정해져 있으므로 확률(0에서 1 사이의 값)을 매우 많이 더하다보면 컴퓨터의 표현 가능 수를 넘어 정확도를 장담할 수 없게 됩니다. 하지만 덧셈의 경우 이러한 위험이 줄어들게 됩니다. 또한, 경사하강법으로 가능도를 최적화할 때 미분 연산을 사용하게 되는데, 로그가능도를 사용하면 연산량을 $O(n^2)$에서 $O(n)$으로 줄일 수 있습니다.   
이러한 계산은 로그가능도를 최적화하는 모수 $\theta$는 가능도를 최적화하는 MLE가 된다는 전제를 바탕으로 하고 있으며, 대게 손실함수의 경우 경사하강법을 사용하므로 음의 로그가능도를 최적화하게 됩니다.   

예제) 정규분포를 따르는 확률변수 X로부터 독립적인 표본 ${x_1, x_2, ... , x_n}$을 얻었을 때 MLE로 모수인 평균과 분산을 추정   
> $\hat{\theta}_{MLE} = argmax_\theta L(\theta;x) = argmax_{\mu,\sigma^2} P(x|\mu,\sigma^2)$
>
> $log L(\theta;X) =  \sum_{i=1}^n log P(x_i|\theta) = \sum_{i=1}^n log \frac{1}{\sqrt{2\pi\sigma^2}} e^{- \frac{|x_i - \mu|^2}{2\sigma^2}}$
> $= - \frac{n}{2} log 2\pi\sigma^2 -  \sum_{i=1}^n  \frac{|x_i - \mu|^2}{2\sigma^2}$
> 
> $\theta = (\mu,\sigma)$에 대해 위 식을 미분해서 두 미분이 모두 0이 되는 $\mu, \sigma$를 찾으면 가능도를 최대화하게 됩니다.  
> $0 =  \frac{ \partial logL}{ \partial \mu} = \sum_{i=1}^n  \frac{x_i - \mu}{\sigma^2}$
> $0 =  \frac{ \partial logL}{ \partial \sigma} = - \frac{n}{\sigma}  +  \frac{n}{\sigma^3}  \sum_{i=1}^n  |x_i - \mu|^2$
> 이를 통해 
>  $\hat{\mu}_{MLE} =  \frac{1}{n}  \sum_{i=1}^n x_i$ 와
>  $\hat{\sigma}^2_{MLE} =  \frac{1}{n}  \sum_{i=1}^n (x_i - \mu)^2$ 로 모수를 추정할 수 있습니다. 

예제2) 카테고리 분포 Multinoulli(x;$p_1, ... , p_d$)를 따르는 확률변수 X로부터 독립적인 표본 ${x_1, ..., x_n}$을 얻었을 때 MLE로 모수를 추정   
> $\hat{\theta}_{MLE} = argmax_{p_1, ..., p_d} L(\theta;x) = argmax_{p_1, ..., p_d} log ( \prod_{i=1}^n  \prod_{k=1}^d p^{x_{i,k}}_k)$  with  $\sum_{k=1}^d p_k = 1$
> $log ( \prod_{i=1}^n  \prod_{k=1}^d p^{x_{i,k}}_k) = \sum_{k=1}^d ( \sum_{i=1}^n x_{i,k} ) log p_k$  
> since $n_k = \sum_{i=1}^n x_{i,k}$ we get $log ( \prod_{i=1}^n  \prod_{k=1}^d p^{x_{i,k}}_k) = \sum_{k=1}^d n_k log p_k$ with $\sum_{k=1}^d p_k = 1$
> 위 식에서 오른쪽 제약식을 만족하면서 왼쪽 목적식을 최대화하는 값이 MLE입니다. 
> 최적화 문제를 풀기 위해 라그랑주 승수법을 이용합니다. 
> 
> $L(p_1, ...,pk,\lambda) = \sum_{k=1}^d n_k log p_k + \lambda (1- \sum_k p_k)$
> 여기서 미분값을 0으로 만드는 모수 $p_k$를 구해보면
> $0 = \frac{\partial L}{\partial p_k} = \frac{n_k}{p_k} - \lambda$ 와
> $0 = \frac{\partial L}{\partial \lambda} = 1 - \sum_{k=1}^d p_k$ 로부터
> $p_k = \frac{n_k}{\sum_{k=1}^d n_k} = \frac{n_k}{n}$
> 즉, 카테고리분포에서 MLE는 경우의 수를 세어 비율을 구하는 것임을 알 수 있습니다.

**딥러닝에서의 MLE**    
기계학습 모델에서 MLE를 적용하면, 딥러닝 모델의 가중치를 $\theta = (W^{(1)}, ..., W^{(L)})$라고 표기했을 때 분류 문제에서 softmax 벡터는 카테고리분포의 모수 $(p_1, ..., p_k)$를 모델링합니다. 또한, one-hot 벡터로 표현한 정답레이블 y=$(y_1, ..., y_K)$을 관찰데이터로 이용해 확률분포인 softmax 벡터의 로그가능도를 최적화할 수 있습니다.   
> $\hat{\theta}_{MLE} = argmax_\theta \frac{1}{n} \sum_{i=1}^n \sum_{k=1}^K y_{i,k} log(MLP_\theta (x_i)_k)$

#### 확률분포의 거리
기계학습에서 사용되는 손실함수들은 모델이 학습하는 확률분포와 데이터에서 관찰되는 확률분포의 거리를 통해 유도합니다.    
- 총변동 거리 (Total Variation Distance, TV)
- 바슈타인 거리 (Wasserstein Distance)
- 쿨백-라이블러 발산 (Kullback-Leibler Divergence, KL)

위와 같은 함수들을 사용하여 데이터 공간에 두 개의 확률분포 $P(x), Q(x)$ 사이의 거리를 계산합니다.   
그 중 KL divergence가 많이 적용되는데, 이에 대해 알아보도록 하겠습니다.
> 이산확률변수 : $KL(P||Q) = \sum_{x \in X} P(x) log(\frac{P(x)}{Q(x)})$
> 연속확률변수 : $KL(P||Q) = \int_{X} P(x) log(\frac{P(x)}{Q(x)})dx$
> 위 식으로부터 아래와 같이 분해할 수 있습니다.     
> $KL(P||Q) = -E_{x - P(x)} [log Q(x)] + E_{x - P(x)} [log P(x)]$
> 여기서 Q(x)항은 크로스 엔트로피, P(x)항은 엔트로피라고 불리며 이 두 항으로 이루어진 KL 발산을 최소화 하는 값으로부터 MLE를 구할 수 있습니다. 
> 여기서 P는 분류 문제에서의 정답레이블, Q는 모델 예측입니다. 

<br>

## 피어세션
---
이번   

<br>

## 개인 학습
---
