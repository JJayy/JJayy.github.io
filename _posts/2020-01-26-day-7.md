---
layout: post
title: Day7
hide_title: False
excerpt: Boost Camp AI Tech - DAY 7
toc: true
toc_sticky: true
# toc_label: Category
feature-img: assets/img/feature-img/story.jpeg
author: Jay
tags: [Boost camp, AI tech, Day7]
---

# Day 6

#### 진행 사항
  - [x] Mathematics for Artificial Intelligence
    - [x] 3강: 경사하강법 1
    - [x] 4강: 경사하강법 2
  - [x] 피어세션 
  - [x] 개인학습


## 학습 내용
---
금일은 지난 내용에서 배웠던 선형회귀 함수에 대해 더 깊게 알아보면서 동시에 최적화 기법인 경사하강법에 대해 알아보았습니다.   

다음은 강의 내용을 정리한 것입니다.   

<br> 

## 경사하강법(Gradient Descent)
---
경사하강법은 딥러닝에서 가장 많이 이용되는 최적화 도구입니다. 이를 이해하기 위해 미분에 대해 알아보도록 하겠습니다.   

### 미분(Differenciation)
미분은 변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구로써 최적화에서 가장 많이 쓰이는 기법입니다. 파이썬에서는 sympy.diff함수를 사용하여 미분 값을 구할 수 있습니다.   

~~~python
import sympy as sym
from sympy.abc import x

sym.diff(sys.poly(x**2 + 2*x + 3), x)
>>> Poly(2*x + 2, x, domain='ZZ')
~~~

특히 미분 값은 함수 f의 주어진 점 (x, f(x))에서의 접선의 기울기를 나타내기도 하는데, 이러한 성질을 이용하여 극대값 또는 극소값을 위치를 예상할 수 있습니다. 극대값을 가지는 위로 볼록한 함수의 경우, 기울기가 양수일 때는 극대값이 x보다 큰 값에 위치하며, 기울기가 음수일 때는 극대값이 x보다 작은 값에 위치합니다. 반대의 경우인 극소값을 가지는 아래로 볼록한 함수의 경우에는, 기울기가 양수일 때는 극소값이 x보다 작은 값에 위치하며, 기울기가 음수일 때는 극소값이 x보다 큰 값에 위치합니다.   
이와 값은 성질을 이용하면, x에 미분값(기울기)를 더하면 기울기가 양수이던 음수이던 f(x) < f(x+기울기)라는 것을 알 수 있습니다. 이를 통해 극대값을 찾아갈 수 있습니다. 이를 경사상승법(gradient ascent)라고 합니다. 반대의 경우에도 성립하는데, x에 미분값(기울기)를 빼면 기울기가 양수이던 음수이던 f(x) > f(x+기울기)라는 것을 알 수 있습니다. 이를 경사하강법(gradient ascent)라고 합니다.   
경사상승법이나 하강법을 통해 극값에 도달하게 되면 기울기가 0이 되기 때문에 더 이상 최적화가 진행되지 않으며, 목적함수의 최적화(극대화 또는 극소화)가 끝나게 됩니다.   
~~~python
 # input : gradient(미분계산 함수), init(시작점), lr(학습률), eps(알고리즘 종료조건)
 # output : var(목적함수)
var = init
grad = gradient(var)
while(abs(grad) > eps): #컴퓨터로 계산할 때는 미분값이 0이 되는 것이 불가능하기 때문에 eps보다 작을 때 종료하는 조건 필요
    var = var - lr*grad
    grad = gradient(var)
~~~
~~~python
import sympy as sym
from sympy.abc import x

def func(val):
    fun = sym.poly(x**2 + 2*x + 3)
    return fun.subs(x, val), fun

def func_gradient(fun, val):
    _, function = fun(val)
    diff = sym.diff(function, x)
    return diff.subs(x, val), diff

def gradient_decent(fun, init_point, lr_rate=1e-2, epsilon=1e-5):
    cnt = 0
    val = init_point
    diff, _ = func_gradient(fun, init_point)
    while np.abs(diff) > epsilon :
        val = val -lr_rate*diff
        diff, _ = func_gradient(fun, val)
        cnt+=1
    
    print("함수: {}, 연산횟수: {}, 최소점: ({},{})".format(fun(val)[1], cnt, val, fun(val)[0])

gradient_descent(fun=func, init_point=np.random.uniform(-2,2))
>>> 함수: Poly(x**2 + 2*x + 3, x, domain='ZZ'), 연산횟수: 636, 최소점: (-0.999995047967832, 2.0000000002452)
~~~

#### 변수가 벡터일 경우
벡터가 입력인 다변수 함수의 경우에는 각각의 변수에 따라 편미분(partial differentiation)을 사용합니다.   
기존의 (f(x+h)-f(x))/h에서 i번째 값만 1이고 나머지는 0인 단위 벡터를 나타네는 e_i가 함께 표시되어 (f(x+h*e_i)-f(x))/h 로 표시할 수 있습니다. (h는 0으로 수렴)   
~~~python
import sympy as sym
from sympy.abc import x, y

sym.diff(sym.poly(x**2 + 2*x*y + 3) + sym.cos(x +2*y), x)
>>> 2*x + 2*y - sin(x + 2*y)
~~~

**그레디언트 벡터(gradient vector)**
앞서 진행한 편미분을 각 변수별로 계산한 벡터를 그레디언트 벡터라고 합니다.   
기존의 미분값을 나타내는 f'(x)가 아니라 그레디언트 벡터는 역삼각형 기호를 사용하여 (nabla)f로 나타냅니다.   

이러한 그레디언트 벡터는 f(x,y)에 대하여 (x, y, z) 공간에 나타낼 수 있는데, 3차원 공간에서의 볼록 함수로 나타나게 됩니다. 이를 쉽게 이해하기 위해서는 이를 다시 2차원 공간에 대응시켜 등고선(contour)로 표현하여 이해할 수 있습니다. 기존의 그레디언트 벡터 (nabla)f는 앞서 기울기가 가졌던 특성처럼 기존의 값에 미분값을 더하면 최대값, 미분값을 빼면 최소값을 얻을 수 있습니다. 대게의 경우 우리는 최소값을 찾기 때문에 -(nabla)f 값을 이용합니다.   

~~~python
import sympy as sym
from sympy.abc import x, y

def eval_(fun, val):
    val_x, val_y = val
    fun_eval = fun.subs(x, val_x).subs(y, val_y)
    return fun_eval

def func_multi(val):
    x_, y_ = val
    fun = sym.poly(x**2 + 2*y**2)
    return eval_(func, [x_, y_]), func

def func_gradient(fun, val):
    x_, y_ = val
    _, function = fun(val)
    diff_x = sym.diff(function, x)
    diff_y = sym.diff(function, y)
    grad_vec = np.array([eval_(diff_x, [x_, y_]), eval_(diff_y, [x_, y_])], dtype=float)
    return grad_vec, [diff_x, diff_y]

def gradient_decent(fun, init_point, lr_rate=1e-2, epsilon=1e-5):
    cnt = 0
    val = init_point
    diff, _ = func_gradient(fun, val)
    while np.linalg.norm(diff) > epsilon :
        val = val -lr_rate*diff
        diff, _ = func_gradient(fun, val)
        cnt+=1
    
    print("함수: {}, 연산횟수: {}, 최소점: ({},{})".format(fun(val)[1], cnt, val, fun(val)[0])

pt=[np.random.uniform(-2,2), np.random.uniform(-2,2)]
gradient_descent(fun=func_multi, init_point=pt)
>>> 함수: Poly(x**2 + 2*y**2, x, y, domain='ZZ'), 연산횟수: 606, 최소점: ([4.95901570e-06 2.88641061e-11], 2.45918366929856E-11)
~~~


### 선형회귀 계수
앞선 수업에서 np.linalg.pinv를 이용하여 역행렬을 통해(n>=m인 경우) 데이터를 선형모델로 해석하는 선형회귀식을 찾을 수 있었습니다. 경사하강법을 이용하면 역행렬을 이용하지 않고 선형모델을 찾을 수 있습니다.   
선형회귀의 목적식은 L2-norm인 ||y-X*beta|| 이고 이를 최소화하는 beta를 찾아야하므로 (nabla)||y-Xbeta||를 구해야 합니다. 이를 계산하면 -(XT_k(y-Xbeta))/(n||y-Xbeta)를 얻을 수 있습니다. 앞서 -(nabla)f를 이용하여 경사하강법을 이용한 것 처럼 그레디언트 벡터 역시 -를 붙여 사용해야합니다. 즉, (XT_k(y-Xbeta))/(n||y-Xbeta)를 사용하게 되고 여기에 학습률인 람다를 곱하여 사용합니다. 
이를 더 간단히 하기 위해 (nabla)||y-Xbeta||의 제곱으로 나타낼 수 있는데, 앞선 식을 (2/n)(XT_k(y-Xbeta))로 간단하게 나타낼 수 있습니다. 제곱을 해도 최솟값을 가지기 위해 beta는 바뀌지 않기 때문에 이와 같이 나타낼 수 있습니다. 

**경사하강법 기반 선형회귀 알고리즘**
~~~python
 #input: X, y, lr(학습률), T(학습횟수)
 #output: beta
 #norm(L2-norm을 계산하는 함수)

for t in rang(T):  # 종료조건을 일정 학습횟수로 변경
    error = y - X@beta
    grad = - transpose(X) @ error
    beta = beta - lr*grad
~~~
~~~python
X = np.array([[1,1], [1,2], [2,2], [2,3]])
y = np.dot(X, np.array([1,2])) + 3

beta_gd = [10.1, 15.1, -6.5] # 최적화 시작점 --- [1, 2, 3]으로 수렴해야 함
X_ = np.array([np.append(x,[1]) for x in X]) #intercept항(y절편) 추가

for t in range(5000):
    error = y - X_ @ beta_gd
    # error = error / np.linalg.norm(error)
    grad = - np.transpose(X_) @ error
    beta_gd = beta_gd - 0.01 * grad

print(beta_gd)
>>> [1.00000367 1.99999949 2.99999516]
~~~
이 과정을 통해 역행렬을 구하지 않고 경사하강법 알고리즘을 적용할 수 있습니다. 하지만 이 경우 위 예시에서 5000번으로 지정한 학습횟수나 0.01로 지정한 학습률이 중요한 hyperparameter가 됩니다. 적절하지 않은 학습횟수나 학습률은 전혀 다른 값을 반환하기 때문에 주의가 필요합니다. 이와 같은 값들은 실험적으로 구할 수 있습니다.   

이론적으로 경사하강법은 미분가능하고 볼록(convex)한 함수에 대해서는 적절한 학습횟수와 학습률을 선택하였을 때 수렴이 보장되어 있습니다. 특히 선형회귀의 경우 목적식인 L2-norm, 즉 ||y-Xbeta||는 회귀계수 beta에 대해 볼록함수이기 때문에 알고리즘을 충분히 돌리면 수렴이 보장됩니다. 하지만 비선형회귀 문제의 경우 목적식이 볼록하지 않을 수 있으므로 수렴이 항상 보장되지는 않습니다. 특히 딥러닝을 사용하는 경우 목적식은 대부분 볼록함수가 아니므로 주의가 필요합니다.   

**확률적 경사하강법(SGD)**
stochastic gradient descent(SGD)라고 불리는 확률적 경사하강법은 모든 데이터 대신 데이터 한 개 또는 일부를 활용하여 목적식을 업데이트 합니다. 앞서 비선형회귀 문제와 같이 볼록이 아닌(non-convex) 목적식은 이와 같은 SGD를 통해 최적화할 수 있습니다. 특히나 SGD는 데이터 전체가 아닌 일부를 가지고 패러미터를 업데이트하기 때문에 연산 자원을 좀 더 효율적으로 활용하는데 도움이 됩니다.   
전체가 아닌 일부의 데이터를 미니배치(mini-batch)라고 하는데, 이와 같은 미니배치는 확률적으로 선택하므로 목적식의 모양이 전체 데이터의 목적식 모양에서 바뀌게 됩니다. 때문에 최솟값이 아닌 지점에서 기울기가 0이어서 묶이는 곳을 탈출할 수 있고, 실질적인 최솟값을 찾을 수 있습니다. 더불어서 일부의 데이터만 사용하는 것이 하드웨어의 리소스를 사용하는데에 이점도 있습니다. 이와 같은 이유로 SGD는 일반적인 경사하강법보다 머신러닝 학습에 더 효율적입니다.   

<br>

## 피어세션
---
금일을 경사하강법(매운맛) 강의에서 further question으로 제공되었던 그레디언트 벡터를 유도하는 것에 대해 이야기해 보았습니다. ||y-Xbeta|| 자체를 미분하는 것은 어렵지 않았지만, 이를 시그마를 활용하여 나타내었을 때, 미분 후 다시 변환하는 것에 있어 모두가 어려움을 겪고 있었습니다. 슬랙에 다른 캠퍼분이 올려주신 유도 과정을 통해 좀 더 확실하게 이해할 수 있었습니다. 또한, 이 과정에서 n으로 나누는 것에 대해 RMSE(Root Mean Squared Error)를 사용하여 나타낸 표현이기 때문이라는 것을 알게 되었습니다.    
이 외에도 batch 사이즈나 구하고자 하는 beta의 이니셜 값, 즉 최적화를 시작하는 시작점을 어떻게 잡는지에 대해서도 논의를 하였고, 이는 실험적으로 구하는 방법 밖에 없다는 결론을 얻게 되었습니다.     

<br>

## 개인 학습
---
앞서 further quetion에서 그레디언트 벡터를 유도하는데 어려움이 있어 벡터와 행렬의 미분에 대해 공부하였습니다.   
다음 블로그를 참고하였습니다.
[(링크)](https://darkpgmr.tistory.com/141){:target="_blank"}
