---
layout: post
title: Day16
hide_title: False
excerpt: Boost Camp AI Tech - DAY 16
use_math: true
toc: true
toc_sticky: true
# toc_label: Category
feature-img: assets/img/feature-img/story.jpeg
author: Jay
tags: [Boost camp, AI tech, Day16]
---

# Day 16

## 학습 내용
  - [x] NLP
    - [x] Bag-of-Words
    - [x] Word Embedding

<br> 

## 개인 학습
---
### Bag-of-Words Representation
나타내고자 하는 문장들을 단어 단위로 tokenization시켜 단어의 뭉치(bag-of-words)를 만듭니다. 
> 예시
> Example sentences: "John really really loves this movie", "Jane really likes this song"
> Vocabulary: {"John", "really", "loves", "this", "movie", "Jane", "likes", "song"}

위 예시처럼 만들어진 카테고리형 variable인 vocabulary를 one-hot vector로 인코딩하여 표현합니다. 이를 위해 단어의 개수 만큼의 차원을 가진 벡터를 설정하여 나타낼 수 있습니다. 
> 예시
> John : [1 0 0 0 0 0 0 0]
> really : [0 1 0 0 0 0 0 0]
> loves : [0 0 1 0 0 0 0 0]
> ...

이를 통해 각각의 벡터 간의 유클리드 거리가 $\sqrt2$로 일정하며, 유사도인 내적값 cosine similarity가 0임을 알 수 있습니다. 즉 각각의 단어가 동일한 관계를 가지도록 표현된 것입니다.   

이를 활용하여 앞서 보았던 예시 문장을 벡터로 표현하면 다음과 같습니다.
> 각 단어의 one-hot 벡터를 더하여 표현
> "John really really loves this movie" [1 2 1 1 1 0 0 0]
> "Jane really likes this song" [0 1 0 1 0 1 1 1]

### Bag-of-Words for Document Classification: NaiveBayes Classifier
Bag-of-words로 표현된 document를 정해진 카테고리나 분류로 classification하는 방법 중 하나인 NaiveBayes Classifier에 대해 알아보도록 하겠습니다.   

이름에서 알 수 있듯이 Bayes' Rule을 사용하여 class에 속할 확률을 계산합니다. 
> For a document $d$ and a class $c$
> $c_{MAP} = argmax_{c \in C} P(c|d)$  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #특정 document인 d가 주어졌을 때 특정 클래스 c에 속할 확률
> by Bayes' Rule $argmax_{c \in C} \frac{P(d|c)P(c)}{P(d)}$ #$P(d)$는 주어진 document로 상수값을 가지므로 생략 가능
> Thus, $c_{MAP} = argmax_{c \in C} P(d|c)P(c)$      
> 
> 여기서 $P(d|c)$는 특정 클래스 c가 주어졌을 때 d가 c에 속할 확률로, d는 sequence of words이므로 word w로 나타낼 수 있습니다. 
> $P(d|c) = P(w_1,w_2,...,w_n|c) = \prod_{w_i \in W} P(w_i|c)$로 나타낼 수 있고, 위 식은 다음과 같이 정리됩니다.
> $c_{MAP} = argmax_{c \in C} P(c)\prod_{w_i \in W} P(w_i|c)$  

예시 분류 문제
> Training set
> 1) Image recognition uses convolutional neural networks / class: CV
> 2) Transformer can be used for image classification task / class: CV
> 3) Language modeling uses transformer / class: NLP
> 4) Document classification task is language task / class: NLP
> 
> Test
> Classification task uses transformer / class: ?

우리는 앞서 구한 $c_{MAP} = argmax_{c \in C} P(c)\prod_{w_i \in W} P(w_i|c)$로부터 max값을 가지는 class로 유추가 가능하다는 것을 알고 있습니다. 각각의 확률을 계산하여 max값을 찾아보도록 하겠습니다.   

먼저 Class는 CV와 NLP 두개로 총 4개의 train data에서 각각 두개씩 존재합니다. 이를 통해   
$P(c_{CV}) = P(c_{NLP}) = \frac{2}{4} = \frac{1}{2}$임을 알 수 있습니다.   
더불어 test에 활용되는 문장을 bag-of-words로 나타내면 {Classification, task, uses, transformer} 4개의 단어로 이루어져 있음을 알 수 있고, 앞서 $P(d|c) = \prod_{w_i \in W} P(w_i|c)$로부터 각각의 단어가 각 class에 있을 확률을 구하여 곱하게 됩니다.    
> class가 CV인 문장에서 단어의 수는 총 14개
> {Classification, task, uses, transformer} 각각의 단어는 1번씩 존재하므로
> 각각의 확률은 {$P(w_{"classification"}|c_{CV}),P(w_{"task"}|c_{CV}),P(w_{"uses"}|c_{CV}),P(w_{"transformer"}|c_{CV})$}는 $\frac{1}{14}$로 동일함을 알 수 있습니다.
> 
> 같은 방식으로 class가 NLP인 문장에서 단어의 수는 총 10개
> 각각의 단어는 {1, 2, 1, 1}개 존재하므로
> {$P(w_{"classification"}|c_{NLP}),P(w_{"task"}|c_{NLP}),P(w_{"uses"}|c_{NLP}),P(w_{"transformer"}|c_{NLP})$}는
> 각각 {$\frac{1}{10},\frac{2}{10},\frac{1}{10},\frac{1}{10}$}임을 알 수 있습니다.
> 
> 이를 통해 $P(c_{CV}|d_{test}) = P(d_{test}|c_{CV})P(c_{CV}) = P(c_{CV})\prod_{w_i \in d_{test}} P(w_i|c_{CV})$를 계산해보면
> $\frac{1}{2} \times \frac{1}{14} \times \frac{1}{14} \times \frac{1}{14} \times \frac{1}{14}$임을 알 수 있습니다. 
> 같은 방식으로 $P(c_{NLP}|d_{test})$ = $\frac{1}{2} \times \frac{1}{10} \times \frac{2}{10} \times \frac{1}{10} \times \frac{1}{10}$임을 알 수 있습니다. 
> 
> $c_{MAP}$은 argmax 값을 통해 class를 유추하으로 $d_{test}$는 더 큰 값을 가진 $c_{NLP}$에 속한다는 것을 구할 수 있습니다.









 

<br> 

## 피어 세션
---